_target_: models.AdaHAT

s_max: 400.0
adjust_strategy: ada
alpha: 1e-06
reg:
  _target_: models.regs.MaskSparseReg
  factor: 0.1

log_train_mask: False
log_capacity: True

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

backbone:
  _target_: models.backbones.MaskedMLP
  input_dim: 784
  hidden_dims: [256, 100]
  output_dim: 64

heads:
  _target_: models.heads.HeadsTIL
  input_dim: ${src.model.backbone.output_dim}
