_target_: models.HAT

s_max: 400.0
reg:
  _target_: models.regs.MaskSparseReg
  factor: 0.1

log_train_mask: False
log_capacity: True

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

backbone:
  _target_: models.backbones.MaskedMLP
  input_dim: 784
  hidden_dims: [256, 100]
  output_dim: 64

te_init:
  _target_: models.inits.Uniform
  

heads:
  _target_: models.heads.HeadsTIL
  input_dim: ${model.backbone.output_dim}
